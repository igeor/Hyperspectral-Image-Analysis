{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a629100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xraylib\n",
    "from math import sqrt\n",
    "import torch\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b67973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic with energies of elements\n",
    "dic={ 'Ca':3692,'Mn':5900,'Fe':6405,'Cu':8046,'Au':9713,'Hg':9989,'Pb':10551,'Sr':14165}\n",
    "num_elements=len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a2d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('panagia1_mm.h5', 'r')\n",
    "dset=f['dataset']\n",
    "dEset=f['energies']\n",
    "spectra_p=np.array(dset)\n",
    "energies=np.array(dEset)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('christ.h5', 'r')\n",
    "dset=f['dataset']\n",
    "spectra_c=np.array(dset)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('hand.h5', 'r')\n",
    "dset=f['dataset']\n",
    "spectra_h=np.array(dset)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b46a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=100\n",
    "rows_p=41\n",
    "cols_p=65\n",
    "rows_c=31\n",
    "cols_c=46\n",
    "rows_h=41\n",
    "cols_h=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501f6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_p = spectra_p.astype(np.float32)\n",
    "spectra_c = spectra_c.astype(np.float32)\n",
    "spectra_h = spectra_h.astype(np.float32)\n",
    "spectra_p=np.delete(spectra_p,np.s_[:channels],1)\n",
    "spectra_c=np.delete(spectra_c,np.s_[:channels],1)\n",
    "spectra_h=np.delete(spectra_h,np.s_[:channels],1)\n",
    "\n",
    "energies=np.delete(energies,np.s_[:channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82a0a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f474441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([861, 1948])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5873a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roi function    \n",
    "def fwhm(ev):\n",
    "    print(sqrt(2.47*ev+4400))\n",
    "    return sqrt(2.47*ev+4400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d2bef77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d34daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_p=torch.tensor(spectra_p)\n",
    "X_c=torch.tensor(spectra_c)\n",
    "X_h=torch.tensor(spectra_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c6ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.27226668470861\n",
      "116.27226668470861\n",
      "137.7425134081704\n",
      "137.7425134081704\n",
      "142.1982770641051\n",
      "142.1982770641051\n",
      "155.7999358151344\n",
      "155.7999358151344\n",
      "168.49661717672555\n",
      "168.49661717672555\n",
      "170.50756581454092\n",
      "170.50756581454092\n",
      "174.53071362943544\n",
      "174.53071362943544\n",
      "198.46296883801776\n",
      "198.46296883801776\n",
      "[129, 239, 264, 346, 429, 443, 471, 651]\n",
      "[135, 246, 271, 354, 437, 451, 479, 661]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "key_list=list(dic.keys())\n",
    "min_ch=[]\n",
    "max_ch=[]\n",
    "min_roi_all=[]\n",
    "max_roi_all=[]\n",
    "for element in dic:\n",
    "    min_roi=(dic[element]-fwhm(dic[element])/2)\n",
    "    min_roi_all.append(min_roi)\n",
    "    max_roi=(dic[element]+fwhm(dic[element])/2)\n",
    "    max_roi_all.append(max_roi)\n",
    "    min_ch.append(int((min_roi+960)/20)-channels)\n",
    "    max_ch.append(int((max_roi+960)/20)-channels)\n",
    "print(min_ch)\n",
    "print(max_ch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f535295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861, 1948)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad52bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_p_cube=spectra_p.reshape(rows_p,cols_p,spectra_p.shape[1])\n",
    "spectra_c_cube=spectra_c.reshape(rows_c,cols_c,spectra_c.shape[1])\n",
    "spectra_h_cube=spectra_h.reshape(cols_h,rows_h,spectra_h.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb92076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "f1=plt.figure()\n",
    "plt.imshow(np.sum(spectra_h_cube[:,:,min_ch[2]:max_ch[2]],axis=2))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c81615",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p=np.zeros([len(spectra_p),num_elements])\n",
    "for i in range(len(spectra_p)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_p[i][j]=np.sum(np.sum(spectra_p[i,min_ch[j]:max_ch[j]]))\n",
    "y_c=np.zeros([len(spectra_c),num_elements])\n",
    "for i in range(len(spectra_c)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_c[i][j]=np.sum(np.sum(spectra_c[i,min_ch[j]:max_ch[j]]))\n",
    "y_h=np.zeros([len(spectra_h),num_elements])\n",
    "for i in range(len(spectra_h)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_h[i][j]=np.sum(np.sum(spectra_h[i,min_ch[j]:max_ch[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b180b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_cube=y_p.reshape(rows_p,cols_p,num_elements)\n",
    "y_c_cube=y_c.reshape(rows_c,cols_c,num_elements)\n",
    "y_h_cube=y_h.reshape(cols_h,rows_h,num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4002bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=(100,),activation='relu',solver='adam',verbose=True,max_iter=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867b21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8258.49851417\n",
      "Iteration 2, loss = 6266.53834506\n",
      "Iteration 3, loss = 4618.82846787\n",
      "Iteration 4, loss = 3211.04459452\n",
      "Iteration 5, loss = 2140.27677113\n",
      "Iteration 6, loss = 1358.00062015\n",
      "Iteration 7, loss = 847.54109457\n",
      "Iteration 8, loss = 587.78388711\n",
      "Iteration 9, loss = 471.81187792\n",
      "Iteration 10, loss = 401.35775360\n",
      "Iteration 11, loss = 321.65105503\n",
      "Iteration 12, loss = 249.29700313\n",
      "Iteration 13, loss = 200.83260531\n",
      "Iteration 14, loss = 166.68337772\n",
      "Iteration 15, loss = 132.30865162\n",
      "Iteration 16, loss = 110.20620172\n",
      "Iteration 17, loss = 92.44517698\n",
      "Iteration 18, loss = 79.16274224\n",
      "Iteration 19, loss = 69.22756525\n",
      "Iteration 20, loss = 61.74514374\n",
      "Iteration 21, loss = 56.51142849\n",
      "Iteration 22, loss = 52.06902907\n",
      "Iteration 23, loss = 48.07013131\n",
      "Iteration 24, loss = 44.49752408\n",
      "Iteration 25, loss = 41.31159918\n",
      "Iteration 26, loss = 38.69481845\n",
      "Iteration 27, loss = 36.29052943\n",
      "Iteration 28, loss = 33.80575660\n",
      "Iteration 29, loss = 31.91946685\n",
      "Iteration 30, loss = 29.96388658\n",
      "Iteration 31, loss = 28.38577979\n",
      "Iteration 32, loss = 26.72771761\n",
      "Iteration 33, loss = 25.29682211\n",
      "Iteration 34, loss = 24.05249392\n",
      "Iteration 35, loss = 22.87406923\n",
      "Iteration 36, loss = 21.72520304\n",
      "Iteration 37, loss = 20.78431870\n",
      "Iteration 38, loss = 20.01876408\n",
      "Iteration 39, loss = 19.10654534\n",
      "Iteration 40, loss = 18.35920524\n",
      "Iteration 41, loss = 17.55555215\n",
      "Iteration 42, loss = 16.89683924\n",
      "Iteration 43, loss = 16.28461912\n",
      "Iteration 44, loss = 15.62541530\n",
      "Iteration 45, loss = 15.16414911\n",
      "Iteration 46, loss = 14.58012427\n",
      "Iteration 47, loss = 14.13293769\n",
      "Iteration 48, loss = 13.64425742\n",
      "Iteration 49, loss = 13.23412000\n",
      "Iteration 50, loss = 12.80915717\n",
      "Iteration 51, loss = 12.44666715\n",
      "Iteration 52, loss = 12.10537068\n",
      "Iteration 53, loss = 11.80857347\n",
      "Iteration 54, loss = 11.43216316\n",
      "Iteration 55, loss = 11.22650786\n",
      "Iteration 56, loss = 11.00496159\n",
      "Iteration 57, loss = 10.69193954\n",
      "Iteration 58, loss = 10.45146991\n",
      "Iteration 59, loss = 10.21005164\n",
      "Iteration 60, loss = 9.98753898\n",
      "Iteration 61, loss = 9.78926283\n",
      "Iteration 62, loss = 9.57117864\n",
      "Iteration 63, loss = 9.38888532\n",
      "Iteration 64, loss = 9.13179228\n",
      "Iteration 65, loss = 8.98072355\n",
      "Iteration 66, loss = 8.79967771\n",
      "Iteration 67, loss = 8.59752945\n",
      "Iteration 68, loss = 8.49976711\n",
      "Iteration 69, loss = 8.27491266\n",
      "Iteration 70, loss = 8.11328920\n",
      "Iteration 71, loss = 8.00219154\n",
      "Iteration 72, loss = 7.86118340\n",
      "Iteration 73, loss = 7.70412990\n",
      "Iteration 74, loss = 7.61463865\n",
      "Iteration 75, loss = 7.45257529\n",
      "Iteration 76, loss = 7.34865671\n",
      "Iteration 77, loss = 7.24350974\n",
      "Iteration 78, loss = 7.15112645\n",
      "Iteration 79, loss = 6.95023263\n",
      "Iteration 80, loss = 6.87883929\n",
      "Iteration 81, loss = 6.77493133\n",
      "Iteration 82, loss = 6.66891915\n",
      "Iteration 83, loss = 6.53892102\n",
      "Iteration 84, loss = 6.43554585\n",
      "Iteration 85, loss = 6.33652336\n",
      "Iteration 86, loss = 6.24977010\n",
      "Iteration 87, loss = 6.15102154\n",
      "Iteration 88, loss = 6.04572467\n",
      "Iteration 89, loss = 5.94274136\n",
      "Iteration 90, loss = 5.86076995\n",
      "Iteration 91, loss = 5.78909000\n",
      "Iteration 92, loss = 5.77824123\n",
      "Iteration 93, loss = 5.66244814\n",
      "Iteration 94, loss = 5.62474055\n",
      "Iteration 95, loss = 5.60778342\n",
      "Iteration 96, loss = 5.49687994\n",
      "Iteration 97, loss = 5.35915939\n",
      "Iteration 98, loss = 5.23647538\n",
      "Iteration 99, loss = 5.17644405\n",
      "Iteration 100, loss = 5.08403060\n",
      "Iteration 101, loss = 4.97343398\n",
      "Iteration 102, loss = 4.89864593\n",
      "Iteration 103, loss = 4.81056099\n",
      "Iteration 104, loss = 4.77057584\n",
      "Iteration 105, loss = 4.68097609\n",
      "Iteration 106, loss = 4.64306089\n",
      "Iteration 107, loss = 4.57734544\n",
      "Iteration 108, loss = 4.55877098\n",
      "Iteration 109, loss = 4.41202578\n",
      "Iteration 110, loss = 4.34849448\n",
      "Iteration 111, loss = 4.26944797\n",
      "Iteration 112, loss = 4.24360644\n",
      "Iteration 113, loss = 4.17718965\n",
      "Iteration 114, loss = 4.09613046\n",
      "Iteration 115, loss = 4.02955803\n",
      "Iteration 116, loss = 3.96280882\n",
      "Iteration 117, loss = 3.89950571\n",
      "Iteration 118, loss = 3.84978317\n",
      "Iteration 119, loss = 3.85147975\n",
      "Iteration 120, loss = 3.79195868\n",
      "Iteration 121, loss = 3.71264544\n",
      "Iteration 122, loss = 3.63491476\n",
      "Iteration 123, loss = 3.55925520\n",
      "Iteration 124, loss = 3.54188188\n",
      "Iteration 125, loss = 3.53106947\n",
      "Iteration 126, loss = 3.46974193\n",
      "Iteration 127, loss = 3.43188583\n",
      "Iteration 128, loss = 3.32329460\n",
      "Iteration 129, loss = 3.24495405\n",
      "Iteration 130, loss = 3.21885670\n",
      "Iteration 131, loss = 3.18170633\n",
      "Iteration 132, loss = 3.20225339\n",
      "Iteration 133, loss = 3.13253004\n",
      "Iteration 134, loss = 3.08895880\n",
      "Iteration 135, loss = 3.07541432\n",
      "Iteration 136, loss = 2.95303180\n",
      "Iteration 137, loss = 2.94776019\n",
      "Iteration 138, loss = 2.89508889\n",
      "Iteration 139, loss = 2.83555381\n",
      "Iteration 140, loss = 2.78967237\n",
      "Iteration 141, loss = 2.74472975\n",
      "Iteration 142, loss = 2.70382304\n",
      "Iteration 143, loss = 2.65416487\n",
      "Iteration 144, loss = 2.61754366\n",
      "Iteration 145, loss = 2.57546612\n",
      "Iteration 146, loss = 2.60266189\n",
      "Iteration 147, loss = 2.53716927\n",
      "Iteration 148, loss = 2.48100610\n",
      "Iteration 149, loss = 2.44332859\n",
      "Iteration 150, loss = 2.41471771\n",
      "Iteration 151, loss = 2.35378392\n",
      "Iteration 152, loss = 2.34571200\n",
      "Iteration 153, loss = 2.35475386\n",
      "Iteration 154, loss = 2.29587504\n",
      "Iteration 155, loss = 2.26194415\n",
      "Iteration 156, loss = 2.22446494\n",
      "Iteration 157, loss = 2.17605811\n",
      "Iteration 158, loss = 2.16525023\n",
      "Iteration 159, loss = 2.09669140\n",
      "Iteration 160, loss = 2.07587324\n",
      "Iteration 161, loss = 2.04151179\n",
      "Iteration 162, loss = 2.05900235\n",
      "Iteration 163, loss = 2.02204852\n",
      "Iteration 164, loss = 1.98616841\n",
      "Iteration 165, loss = 1.93758668\n",
      "Iteration 166, loss = 1.95445634\n",
      "Iteration 167, loss = 1.92673525\n",
      "Iteration 168, loss = 1.87608708\n",
      "Iteration 169, loss = 1.89056647\n",
      "Iteration 170, loss = 1.82101884\n",
      "Iteration 171, loss = 1.81536637\n",
      "Iteration 172, loss = 1.80346404\n",
      "Iteration 173, loss = 1.78648098\n",
      "Iteration 174, loss = 1.72834650\n",
      "Iteration 175, loss = 1.70072162\n",
      "Iteration 176, loss = 1.67544399\n",
      "Iteration 177, loss = 1.63302324\n",
      "Iteration 178, loss = 1.62016012\n",
      "Iteration 179, loss = 1.61088618\n",
      "Iteration 180, loss = 1.57902236\n",
      "Iteration 181, loss = 1.54167239\n",
      "Iteration 182, loss = 1.51884657\n",
      "Iteration 183, loss = 1.50647055\n",
      "Iteration 184, loss = 1.48102765\n",
      "Iteration 185, loss = 1.50239781\n",
      "Iteration 186, loss = 1.43981507\n",
      "Iteration 187, loss = 1.44618684\n",
      "Iteration 188, loss = 1.43042235\n",
      "Iteration 189, loss = 1.40118988\n",
      "Iteration 190, loss = 1.40451201\n",
      "Iteration 191, loss = 1.37775475\n",
      "Iteration 192, loss = 1.35119623\n",
      "Iteration 193, loss = 1.31797954\n",
      "Iteration 194, loss = 1.30531926\n",
      "Iteration 195, loss = 1.28761971\n",
      "Iteration 196, loss = 1.27455733\n",
      "Iteration 197, loss = 1.26471934\n",
      "Iteration 198, loss = 1.24775363\n",
      "Iteration 199, loss = 1.23397734\n",
      "Iteration 200, loss = 1.19968744\n",
      "Iteration 201, loss = 1.21400362\n",
      "Iteration 202, loss = 1.18260170\n",
      "Iteration 203, loss = 1.14374495\n",
      "Iteration 204, loss = 1.13512795\n",
      "Iteration 205, loss = 1.13803737\n",
      "Iteration 206, loss = 1.11705860\n",
      "Iteration 207, loss = 1.10997320\n",
      "Iteration 208, loss = 1.08831162\n",
      "Iteration 209, loss = 1.08311668\n",
      "Iteration 210, loss = 1.07110953\n",
      "Iteration 211, loss = 1.05846133\n",
      "Iteration 212, loss = 1.03018021\n",
      "Iteration 213, loss = 1.02901950\n",
      "Iteration 214, loss = 0.99682806\n",
      "Iteration 215, loss = 0.98609020\n",
      "Iteration 216, loss = 0.98358189\n",
      "Iteration 217, loss = 0.97992610\n",
      "Iteration 218, loss = 0.96615820\n",
      "Iteration 219, loss = 0.95998188\n",
      "Iteration 220, loss = 0.93588292\n",
      "Iteration 221, loss = 0.92491358\n",
      "Iteration 222, loss = 0.89861312\n",
      "Iteration 223, loss = 0.89546418\n",
      "Iteration 224, loss = 0.87758537\n",
      "Iteration 225, loss = 0.86151346\n",
      "Iteration 226, loss = 0.84642844\n",
      "Iteration 227, loss = 0.83960626\n",
      "Iteration 228, loss = 0.84525958\n",
      "Iteration 229, loss = 0.85767545\n",
      "Iteration 230, loss = 0.83008886\n",
      "Iteration 231, loss = 0.83392454\n",
      "Iteration 232, loss = 0.79766236\n",
      "Iteration 233, loss = 0.78972376\n",
      "Iteration 234, loss = 0.78807951\n",
      "Iteration 235, loss = 0.76361094\n",
      "Iteration 236, loss = 0.74527143\n",
      "Iteration 237, loss = 0.75852333\n",
      "Iteration 238, loss = 0.74299359\n",
      "Iteration 239, loss = 0.73859559\n",
      "Iteration 240, loss = 0.72685565\n",
      "Iteration 241, loss = 0.71572023\n",
      "Iteration 242, loss = 0.70757349\n",
      "Iteration 243, loss = 0.69312028\n",
      "Iteration 244, loss = 0.69034231\n",
      "Iteration 245, loss = 0.67156976\n",
      "Iteration 246, loss = 0.67180821\n",
      "Iteration 247, loss = 0.65687241\n",
      "Iteration 248, loss = 0.63892172\n",
      "Iteration 249, loss = 0.63715984\n",
      "Iteration 250, loss = 0.63492643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251, loss = 0.62100395\n",
      "Iteration 252, loss = 0.60129300\n",
      "Iteration 253, loss = 0.60821377\n",
      "Iteration 254, loss = 0.60378824\n",
      "Iteration 255, loss = 0.60680711\n",
      "Iteration 256, loss = 0.58957090\n",
      "Iteration 257, loss = 0.58031929\n",
      "Iteration 258, loss = 0.56611272\n",
      "Iteration 259, loss = 0.56104230\n",
      "Iteration 260, loss = 0.56240424\n",
      "Iteration 261, loss = 0.56088498\n",
      "Iteration 262, loss = 0.54747144\n",
      "Iteration 263, loss = 0.53552127\n",
      "Iteration 264, loss = 0.53501095\n",
      "Iteration 265, loss = 0.52668839\n",
      "Iteration 266, loss = 0.53021598\n",
      "Iteration 267, loss = 0.53148185\n",
      "Iteration 268, loss = 0.52467050\n",
      "Iteration 269, loss = 0.49880604\n",
      "Iteration 270, loss = 0.48984604\n",
      "Iteration 271, loss = 0.48578257\n",
      "Iteration 272, loss = 0.49031215\n",
      "Iteration 273, loss = 0.48558725\n",
      "Iteration 274, loss = 0.48137201\n",
      "Iteration 275, loss = 0.46024088\n",
      "Iteration 276, loss = 0.46062971\n",
      "Iteration 277, loss = 0.47005202\n",
      "Iteration 278, loss = 0.45749478\n",
      "Iteration 279, loss = 0.46627666\n",
      "Iteration 280, loss = 0.44396533\n",
      "Iteration 281, loss = 0.43309802\n",
      "Iteration 282, loss = 0.42802852\n",
      "Iteration 283, loss = 0.42765927\n",
      "Iteration 284, loss = 0.41871934\n",
      "Iteration 285, loss = 0.41186435\n",
      "Iteration 286, loss = 0.40475726\n",
      "Iteration 287, loss = 0.40641712\n",
      "Iteration 288, loss = 0.39490305\n",
      "Iteration 289, loss = 0.39485526\n",
      "Iteration 290, loss = 0.38622330\n",
      "Iteration 291, loss = 0.38309383\n",
      "Iteration 292, loss = 0.38252454\n",
      "Iteration 293, loss = 0.37225167\n",
      "Iteration 294, loss = 0.36491859\n",
      "Iteration 295, loss = 0.36501715\n",
      "Iteration 296, loss = 0.36330618\n",
      "Iteration 297, loss = 0.35672092\n",
      "Iteration 298, loss = 0.36042964\n",
      "Iteration 299, loss = 0.35838193\n",
      "Iteration 300, loss = 0.36054205\n",
      "Iteration 301, loss = 0.34083745\n",
      "Iteration 302, loss = 0.33884130\n",
      "Iteration 303, loss = 0.33552584\n",
      "Iteration 304, loss = 0.32958874\n",
      "Iteration 305, loss = 0.32232822\n",
      "Iteration 306, loss = 0.32762740\n",
      "Iteration 307, loss = 0.32843454\n",
      "Iteration 308, loss = 0.32247554\n",
      "Iteration 309, loss = 0.31420188\n",
      "Iteration 310, loss = 0.30826799\n",
      "Iteration 311, loss = 0.30105768\n",
      "Iteration 312, loss = 0.29443047\n",
      "Iteration 313, loss = 0.29700533\n",
      "Iteration 314, loss = 0.30738526\n",
      "Iteration 315, loss = 0.29666987\n",
      "Iteration 316, loss = 0.29375533\n",
      "Iteration 317, loss = 0.28157486\n",
      "Iteration 318, loss = 0.27470808\n",
      "Iteration 319, loss = 0.27388854\n",
      "Iteration 320, loss = 0.27958727\n",
      "Iteration 321, loss = 0.26623638\n",
      "Iteration 322, loss = 0.26550470\n",
      "Iteration 323, loss = 0.26079239\n",
      "Iteration 324, loss = 0.25974506\n",
      "Iteration 325, loss = 0.25283877\n",
      "Iteration 326, loss = 0.26028722\n",
      "Iteration 327, loss = 0.24744223\n",
      "Iteration 328, loss = 0.24879446\n",
      "Iteration 329, loss = 0.24583471\n",
      "Iteration 330, loss = 0.23805305\n",
      "Iteration 331, loss = 0.24299807\n",
      "Iteration 332, loss = 0.24115630\n",
      "Iteration 333, loss = 0.23870126\n",
      "Iteration 334, loss = 0.23529445\n",
      "Iteration 335, loss = 0.23432489\n",
      "Iteration 336, loss = 0.23348056\n",
      "Iteration 337, loss = 0.22873898\n",
      "Iteration 338, loss = 0.22301385\n",
      "Iteration 339, loss = 0.21883960\n",
      "Iteration 340, loss = 0.22301412\n",
      "Iteration 341, loss = 0.21658294\n",
      "Iteration 342, loss = 0.21041161\n",
      "Iteration 343, loss = 0.20722875\n",
      "Iteration 344, loss = 0.20369402\n",
      "Iteration 345, loss = 0.19824208\n",
      "Iteration 346, loss = 0.20119423\n",
      "Iteration 347, loss = 0.19423954\n",
      "Iteration 348, loss = 0.19491924\n",
      "Iteration 349, loss = 0.19482271\n",
      "Iteration 350, loss = 0.19198491\n",
      "Iteration 351, loss = 0.19025809\n",
      "Iteration 352, loss = 0.18882984\n",
      "Iteration 353, loss = 0.18957569\n",
      "Iteration 354, loss = 0.18585706\n",
      "Iteration 355, loss = 0.18048984\n",
      "Iteration 356, loss = 0.18857404\n",
      "Iteration 357, loss = 0.18275066\n",
      "Iteration 358, loss = 0.17634433\n",
      "Iteration 359, loss = 0.17440380\n",
      "Iteration 360, loss = 0.17116624\n",
      "Iteration 361, loss = 0.17136312\n",
      "Iteration 362, loss = 0.17238847\n",
      "Iteration 363, loss = 0.17797543\n",
      "Iteration 364, loss = 0.16916712\n",
      "Iteration 365, loss = 0.16813474\n",
      "Iteration 366, loss = 0.16834020\n",
      "Iteration 367, loss = 0.16514950\n",
      "Iteration 368, loss = 0.16668034\n",
      "Iteration 369, loss = 0.16084701\n",
      "Iteration 370, loss = 0.16481652\n",
      "Iteration 371, loss = 0.15672925\n",
      "Iteration 372, loss = 0.15233985\n",
      "Iteration 373, loss = 0.15098066\n",
      "Iteration 374, loss = 0.14517223\n",
      "Iteration 375, loss = 0.14541484\n",
      "Iteration 376, loss = 0.14088810\n",
      "Iteration 377, loss = 0.14121545\n",
      "Iteration 378, loss = 0.14373326\n",
      "Iteration 379, loss = 0.15289344\n",
      "Iteration 380, loss = 0.14515119\n",
      "Iteration 381, loss = 0.14144700\n",
      "Iteration 382, loss = 0.13331807\n",
      "Iteration 383, loss = 0.13473411\n",
      "Iteration 384, loss = 0.13255979\n",
      "Iteration 385, loss = 0.13031344\n",
      "Iteration 386, loss = 0.12799617\n",
      "Iteration 387, loss = 0.12664325\n",
      "Iteration 388, loss = 0.12975932\n",
      "Iteration 389, loss = 0.12629685\n",
      "Iteration 390, loss = 0.12559737\n",
      "Iteration 391, loss = 0.13121114\n",
      "Iteration 392, loss = 0.12595295\n",
      "Iteration 393, loss = 0.12498302\n",
      "Iteration 394, loss = 0.12172935\n",
      "Iteration 395, loss = 0.12304344\n",
      "Iteration 396, loss = 0.12001834\n",
      "Iteration 397, loss = 0.11259037\n",
      "Iteration 398, loss = 0.11890414\n",
      "Iteration 399, loss = 0.11146198\n",
      "Iteration 400, loss = 0.11238767\n",
      "Iteration 401, loss = 0.11257057\n",
      "Iteration 402, loss = 0.11018450\n",
      "Iteration 403, loss = 0.10615646\n",
      "Iteration 404, loss = 0.10585109\n",
      "Iteration 405, loss = 0.10809465\n",
      "Iteration 406, loss = 0.10678906\n",
      "Iteration 407, loss = 0.10328080\n",
      "Iteration 408, loss = 0.10320753\n",
      "Iteration 409, loss = 0.10265842\n",
      "Iteration 410, loss = 0.09780585\n",
      "Iteration 411, loss = 0.09793546\n",
      "Iteration 412, loss = 0.09857034\n",
      "Iteration 413, loss = 0.09515991\n",
      "Iteration 414, loss = 0.09364943\n",
      "Iteration 415, loss = 0.09315090\n",
      "Iteration 416, loss = 0.09113761\n",
      "Iteration 417, loss = 0.09385923\n",
      "Iteration 418, loss = 0.09397954\n",
      "Iteration 419, loss = 0.09825088\n",
      "Iteration 420, loss = 0.09842059\n",
      "Iteration 421, loss = 0.09146502\n",
      "Iteration 422, loss = 0.08584355\n",
      "Iteration 423, loss = 0.08475030\n",
      "Iteration 424, loss = 0.08678252\n",
      "Iteration 425, loss = 0.08795915\n",
      "Iteration 426, loss = 0.09174369\n",
      "Iteration 427, loss = 0.08513422\n",
      "Iteration 428, loss = 0.08292532\n",
      "Iteration 429, loss = 0.08004623\n",
      "Iteration 430, loss = 0.07842285\n",
      "Iteration 431, loss = 0.07831767\n",
      "Iteration 432, loss = 0.07815401\n",
      "Iteration 433, loss = 0.07634803\n",
      "Iteration 434, loss = 0.07333776\n",
      "Iteration 435, loss = 0.07667680\n",
      "Iteration 436, loss = 0.07574921\n",
      "Iteration 437, loss = 0.08108995\n",
      "Iteration 438, loss = 0.07774524\n",
      "Iteration 439, loss = 0.07630656\n",
      "Iteration 440, loss = 0.07244207\n",
      "Iteration 441, loss = 0.07098752\n",
      "Iteration 442, loss = 0.07143557\n",
      "Iteration 443, loss = 0.06949062\n",
      "Iteration 444, loss = 0.06867595\n",
      "Iteration 445, loss = 0.07179412\n",
      "Iteration 446, loss = 0.06832948\n",
      "Iteration 447, loss = 0.06588176\n",
      "Iteration 448, loss = 0.06642652\n",
      "Iteration 449, loss = 0.06865995\n",
      "Iteration 450, loss = 0.06649607\n",
      "Iteration 451, loss = 0.06773519\n",
      "Iteration 452, loss = 0.06489345\n",
      "Iteration 453, loss = 0.06455675\n",
      "Iteration 454, loss = 0.06490862\n",
      "Iteration 455, loss = 0.06505558\n",
      "Iteration 456, loss = 0.06339167\n",
      "Iteration 457, loss = 0.06713255\n",
      "Iteration 458, loss = 0.06702354\n",
      "Iteration 459, loss = 0.06014729\n",
      "Iteration 460, loss = 0.06048211\n",
      "Iteration 461, loss = 0.06002494\n",
      "Iteration 462, loss = 0.05750547\n",
      "Iteration 463, loss = 0.05564286\n",
      "Iteration 464, loss = 0.05554072\n",
      "Iteration 465, loss = 0.05742623\n",
      "Iteration 466, loss = 0.05349631\n",
      "Iteration 467, loss = 0.05725518\n",
      "Iteration 468, loss = 0.05461640\n",
      "Iteration 469, loss = 0.05539945\n",
      "Iteration 470, loss = 0.05496612\n",
      "Iteration 471, loss = 0.05844320\n",
      "Iteration 472, loss = 0.05556455\n",
      "Iteration 473, loss = 0.05481793\n",
      "Iteration 474, loss = 0.05389426\n",
      "Iteration 475, loss = 0.05259461\n",
      "Iteration 476, loss = 0.04976825\n",
      "Iteration 477, loss = 0.05045851\n",
      "Iteration 478, loss = 0.04974096\n",
      "Iteration 479, loss = 0.05151937\n",
      "Iteration 480, loss = 0.05103878\n",
      "Iteration 481, loss = 0.04993295\n",
      "Iteration 482, loss = 0.04760234\n",
      "Iteration 483, loss = 0.04496364\n",
      "Iteration 484, loss = 0.04442388\n",
      "Iteration 485, loss = 0.04572111\n",
      "Iteration 486, loss = 0.04688410\n",
      "Iteration 487, loss = 0.04666461\n",
      "Iteration 488, loss = 0.04414514\n",
      "Iteration 489, loss = 0.04327191\n",
      "Iteration 490, loss = 0.04416268\n",
      "Iteration 491, loss = 0.04208630\n",
      "Iteration 492, loss = 0.04373614\n",
      "Iteration 493, loss = 0.04340257\n",
      "Iteration 494, loss = 0.04251445\n",
      "Iteration 495, loss = 0.04160581\n",
      "Iteration 496, loss = 0.04032090\n",
      "Iteration 497, loss = 0.04137873\n",
      "Iteration 498, loss = 0.04036964\n",
      "Iteration 499, loss = 0.03983956\n",
      "Iteration 500, loss = 0.04159771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_h, y_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db06d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x259b27608e0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(regr.loss_curve_[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7aa9c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 29065.32217228\n",
      "Iteration 2, loss = 13611.43165639\n",
      "Iteration 3, loss = 6307.96675691\n",
      "Iteration 4, loss = 2967.18782684\n",
      "Iteration 5, loss = 1182.17284455\n",
      "Iteration 6, loss = 432.57502525\n",
      "Iteration 7, loss = 257.16810740\n",
      "Iteration 8, loss = 190.75018614\n",
      "Iteration 9, loss = 140.40339158\n",
      "Iteration 10, loss = 109.43844617\n",
      "Iteration 11, loss = 90.71681345\n",
      "Iteration 12, loss = 77.93291224\n",
      "Iteration 13, loss = 69.64514239\n",
      "Iteration 14, loss = 62.54706842\n",
      "Iteration 15, loss = 56.89507483\n",
      "Iteration 16, loss = 51.64912719\n",
      "Iteration 17, loss = 47.54488944\n",
      "Iteration 18, loss = 44.23811090\n",
      "Iteration 19, loss = 41.85407203\n",
      "Iteration 20, loss = 39.67580951\n",
      "Iteration 21, loss = 38.10533640\n",
      "Iteration 22, loss = 36.51889163\n",
      "Iteration 23, loss = 35.38178581\n",
      "Iteration 24, loss = 34.35117407\n",
      "Iteration 25, loss = 33.39922241\n",
      "Iteration 26, loss = 32.85181279\n",
      "Iteration 27, loss = 32.07761387\n",
      "Iteration 28, loss = 31.61805354\n",
      "Iteration 29, loss = 30.94990335\n",
      "Iteration 30, loss = 30.46206949\n",
      "Iteration 31, loss = 30.02033232\n",
      "Iteration 32, loss = 29.59910836\n",
      "Iteration 33, loss = 29.10166323\n",
      "Iteration 34, loss = 28.92677215\n",
      "Iteration 35, loss = 28.27553035\n",
      "Iteration 36, loss = 27.77701996\n",
      "Iteration 37, loss = 27.50248604\n",
      "Iteration 38, loss = 27.42637732\n",
      "Iteration 39, loss = 26.90845695\n",
      "Iteration 40, loss = 26.26467035\n",
      "Iteration 41, loss = 25.98528728\n",
      "Iteration 42, loss = 25.66645806\n",
      "Iteration 43, loss = 25.28703728\n",
      "Iteration 44, loss = 25.23121804\n",
      "Iteration 45, loss = 25.07461534\n",
      "Iteration 46, loss = 24.27932592\n",
      "Iteration 47, loss = 24.06191334\n",
      "Iteration 48, loss = 23.63886525\n",
      "Iteration 49, loss = 23.59518901\n",
      "Iteration 50, loss = 22.80263698\n",
      "Iteration 51, loss = 22.47184641\n",
      "Iteration 52, loss = 22.24462710\n",
      "Iteration 53, loss = 22.06502217\n",
      "Iteration 54, loss = 21.60444571\n",
      "Iteration 55, loss = 21.38661599\n",
      "Iteration 56, loss = 20.93122349\n",
      "Iteration 57, loss = 20.66902955\n",
      "Iteration 58, loss = 20.46762344\n",
      "Iteration 59, loss = 19.87987419\n",
      "Iteration 60, loss = 20.13988465\n",
      "Iteration 61, loss = 19.52942090\n",
      "Iteration 62, loss = 19.44072048\n",
      "Iteration 63, loss = 19.21695442\n",
      "Iteration 64, loss = 19.00483678\n",
      "Iteration 65, loss = 18.50021952\n",
      "Iteration 66, loss = 18.32027228\n",
      "Iteration 67, loss = 18.35075380\n",
      "Iteration 68, loss = 17.93942978\n",
      "Iteration 69, loss = 17.66020123\n",
      "Iteration 70, loss = 17.30684048\n",
      "Iteration 71, loss = 16.98548092\n",
      "Iteration 72, loss = 16.71725547\n",
      "Iteration 73, loss = 16.73389864\n",
      "Iteration 74, loss = 16.59562564\n",
      "Iteration 75, loss = 15.91803435\n",
      "Iteration 76, loss = 15.91362657\n",
      "Iteration 77, loss = 15.68088307\n",
      "Iteration 78, loss = 15.21742403\n",
      "Iteration 79, loss = 14.96907212\n",
      "Iteration 80, loss = 14.86529357\n",
      "Iteration 81, loss = 14.73443215\n",
      "Iteration 82, loss = 14.87603025\n",
      "Iteration 83, loss = 14.42234975\n",
      "Iteration 84, loss = 14.28619966\n",
      "Iteration 85, loss = 13.75421654\n",
      "Iteration 86, loss = 13.62722265\n",
      "Iteration 87, loss = 13.33392824\n",
      "Iteration 88, loss = 13.15737342\n",
      "Iteration 89, loss = 13.05881081\n",
      "Iteration 90, loss = 12.94555911\n",
      "Iteration 91, loss = 12.56395224\n",
      "Iteration 92, loss = 12.46511373\n",
      "Iteration 93, loss = 12.22244958\n",
      "Iteration 94, loss = 12.16112098\n",
      "Iteration 95, loss = 12.22210232\n",
      "Iteration 96, loss = 11.84201276\n",
      "Iteration 97, loss = 11.59573626\n",
      "Iteration 98, loss = 11.32280889\n",
      "Iteration 99, loss = 11.20548432\n",
      "Iteration 100, loss = 11.03969018\n",
      "Iteration 101, loss = 11.05435455\n",
      "Iteration 102, loss = 11.04148395\n",
      "Iteration 103, loss = 10.46206752\n",
      "Iteration 104, loss = 10.87094921\n",
      "Iteration 105, loss = 10.42068271\n",
      "Iteration 106, loss = 10.15232953\n",
      "Iteration 107, loss = 9.98932421\n",
      "Iteration 108, loss = 9.77994155\n",
      "Iteration 109, loss = 9.74022670\n",
      "Iteration 110, loss = 9.59446541\n",
      "Iteration 111, loss = 9.23290762\n",
      "Iteration 112, loss = 9.21575491\n",
      "Iteration 113, loss = 9.15015713\n",
      "Iteration 114, loss = 8.94287747\n",
      "Iteration 115, loss = 8.71463706\n",
      "Iteration 116, loss = 9.04616262\n",
      "Iteration 117, loss = 8.70405204\n",
      "Iteration 118, loss = 8.54160968\n",
      "Iteration 119, loss = 8.57821053\n",
      "Iteration 120, loss = 8.33490415\n",
      "Iteration 121, loss = 8.21989361\n",
      "Iteration 122, loss = 8.06157915\n",
      "Iteration 123, loss = 7.83006258\n",
      "Iteration 124, loss = 7.75270639\n",
      "Iteration 125, loss = 7.72251585\n",
      "Iteration 126, loss = 7.42573076\n",
      "Iteration 127, loss = 7.41682639\n",
      "Iteration 128, loss = 7.65319757\n",
      "Iteration 129, loss = 7.49377440\n",
      "Iteration 130, loss = 7.16541901\n",
      "Iteration 131, loss = 7.00734785\n",
      "Iteration 132, loss = 6.86005004\n",
      "Iteration 133, loss = 6.66764279\n",
      "Iteration 134, loss = 6.65720233\n",
      "Iteration 135, loss = 6.68243194\n",
      "Iteration 136, loss = 6.35862382\n",
      "Iteration 137, loss = 6.30889813\n",
      "Iteration 138, loss = 6.27955091\n",
      "Iteration 139, loss = 6.21689128\n",
      "Iteration 140, loss = 6.27457023\n",
      "Iteration 141, loss = 6.63561613\n",
      "Iteration 142, loss = 6.17772974\n",
      "Iteration 143, loss = 6.00373113\n",
      "Iteration 144, loss = 5.96689191\n",
      "Iteration 145, loss = 5.73905031\n",
      "Iteration 146, loss = 5.58740247\n",
      "Iteration 147, loss = 5.43446582\n",
      "Iteration 148, loss = 5.79889078\n",
      "Iteration 149, loss = 5.74961350\n",
      "Iteration 150, loss = 5.36248341\n",
      "Iteration 151, loss = 5.38170408\n",
      "Iteration 152, loss = 5.06961861\n",
      "Iteration 153, loss = 4.98393065\n",
      "Iteration 154, loss = 4.96122247\n",
      "Iteration 155, loss = 5.00489597\n",
      "Iteration 156, loss = 5.00926500\n",
      "Iteration 157, loss = 4.86388545\n",
      "Iteration 158, loss = 4.93844717\n",
      "Iteration 159, loss = 4.66888726\n",
      "Iteration 160, loss = 4.60159116\n",
      "Iteration 161, loss = 4.63902089\n",
      "Iteration 162, loss = 4.40995111\n",
      "Iteration 163, loss = 4.40242563\n",
      "Iteration 164, loss = 4.27326081\n",
      "Iteration 165, loss = 4.14359201\n",
      "Iteration 166, loss = 4.21691294\n",
      "Iteration 167, loss = 4.19923276\n",
      "Iteration 168, loss = 4.10292310\n",
      "Iteration 169, loss = 4.01546446\n",
      "Iteration 170, loss = 3.94126448\n",
      "Iteration 171, loss = 3.90200224\n",
      "Iteration 172, loss = 3.77948557\n",
      "Iteration 173, loss = 3.90861268\n",
      "Iteration 174, loss = 3.96119980\n",
      "Iteration 175, loss = 3.79207623\n",
      "Iteration 176, loss = 3.76099290\n",
      "Iteration 177, loss = 3.71597950\n",
      "Iteration 178, loss = 3.55792557\n",
      "Iteration 179, loss = 3.41092557\n",
      "Iteration 180, loss = 3.39069508\n",
      "Iteration 181, loss = 3.46972421\n",
      "Iteration 182, loss = 3.50976976\n",
      "Iteration 183, loss = 3.37318423\n",
      "Iteration 184, loss = 3.41252795\n",
      "Iteration 185, loss = 3.26439781\n",
      "Iteration 186, loss = 3.12830948\n",
      "Iteration 187, loss = 3.11051619\n",
      "Iteration 188, loss = 3.08577825\n",
      "Iteration 189, loss = 3.08934450\n",
      "Iteration 190, loss = 3.22794320\n",
      "Iteration 191, loss = 3.11876806\n",
      "Iteration 192, loss = 2.99323036\n",
      "Iteration 193, loss = 2.93802499\n",
      "Iteration 194, loss = 2.88798766\n",
      "Iteration 195, loss = 2.86186414\n",
      "Iteration 196, loss = 2.89179283\n",
      "Iteration 197, loss = 2.87319541\n",
      "Iteration 198, loss = 2.85731107\n",
      "Iteration 199, loss = 2.73301221\n",
      "Iteration 200, loss = 2.95107642\n",
      "Iteration 201, loss = 3.01086616\n",
      "Iteration 202, loss = 2.87323594\n",
      "Iteration 203, loss = 2.93288559\n",
      "Iteration 204, loss = 2.87671530\n",
      "Iteration 205, loss = 2.64216169\n",
      "Iteration 206, loss = 2.68747429\n",
      "Iteration 207, loss = 2.80951828\n",
      "Iteration 208, loss = 2.79766603\n",
      "Iteration 209, loss = 2.58364276\n",
      "Iteration 210, loss = 2.41917667\n",
      "Iteration 211, loss = 2.42975640\n",
      "Iteration 212, loss = 2.32068765\n",
      "Iteration 213, loss = 2.27765236\n",
      "Iteration 214, loss = 2.28376756\n",
      "Iteration 215, loss = 2.20829073\n",
      "Iteration 216, loss = 2.20490704\n",
      "Iteration 217, loss = 2.30168811\n",
      "Iteration 218, loss = 2.49801654\n",
      "Iteration 219, loss = 2.20590259\n",
      "Iteration 220, loss = 2.19665007\n",
      "Iteration 221, loss = 2.22072347\n",
      "Iteration 222, loss = 2.28275267\n",
      "Iteration 223, loss = 2.25646705\n",
      "Iteration 224, loss = 2.10808890\n",
      "Iteration 225, loss = 1.99564725\n",
      "Iteration 226, loss = 1.92969864\n",
      "Iteration 227, loss = 2.01553300\n",
      "Iteration 228, loss = 1.93475399\n",
      "Iteration 229, loss = 1.92732451\n",
      "Iteration 230, loss = 1.89726983\n",
      "Iteration 231, loss = 1.81082279\n",
      "Iteration 232, loss = 1.95140364\n",
      "Iteration 233, loss = 1.80385786\n",
      "Iteration 234, loss = 1.75388157\n",
      "Iteration 235, loss = 1.73549322\n",
      "Iteration 236, loss = 1.82985636\n",
      "Iteration 237, loss = 1.91581179\n",
      "Iteration 238, loss = 1.81701117\n",
      "Iteration 239, loss = 1.65724539\n",
      "Iteration 240, loss = 1.74850395\n",
      "Iteration 241, loss = 1.87580388\n",
      "Iteration 242, loss = 1.59475587\n",
      "Iteration 243, loss = 1.57312848\n",
      "Iteration 244, loss = 1.60966329\n",
      "Iteration 245, loss = 1.72522765\n",
      "Iteration 246, loss = 1.65113477\n",
      "Iteration 247, loss = 1.63320739\n",
      "Iteration 248, loss = 1.80158620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 249, loss = 1.97374908\n",
      "Iteration 250, loss = 1.80260729\n",
      "Iteration 251, loss = 1.52801543\n",
      "Iteration 252, loss = 1.60893139\n",
      "Iteration 253, loss = 1.42187953\n",
      "Iteration 254, loss = 1.37464083\n",
      "Iteration 255, loss = 1.45919747\n",
      "Iteration 256, loss = 1.46823355\n",
      "Iteration 257, loss = 1.44616512\n",
      "Iteration 258, loss = 1.34386678\n",
      "Iteration 259, loss = 1.39270730\n",
      "Iteration 260, loss = 1.46362961\n",
      "Iteration 261, loss = 1.40557073\n",
      "Iteration 262, loss = 1.32905669\n",
      "Iteration 263, loss = 1.25995656\n",
      "Iteration 264, loss = 1.35756562\n",
      "Iteration 265, loss = 1.48993701\n",
      "Iteration 266, loss = 1.31163024\n",
      "Iteration 267, loss = 1.29972133\n",
      "Iteration 268, loss = 1.24237943\n",
      "Iteration 269, loss = 1.23917824\n",
      "Iteration 270, loss = 1.23895331\n",
      "Iteration 271, loss = 1.31151344\n",
      "Iteration 272, loss = 1.30247763\n",
      "Iteration 273, loss = 1.23936283\n",
      "Iteration 274, loss = 1.24633557\n",
      "Iteration 275, loss = 1.18500427\n",
      "Iteration 276, loss = 1.18849133\n",
      "Iteration 277, loss = 1.10704061\n",
      "Iteration 278, loss = 1.09507999\n",
      "Iteration 279, loss = 1.21278529\n",
      "Iteration 280, loss = 1.07511101\n",
      "Iteration 281, loss = 1.08897188\n",
      "Iteration 282, loss = 1.21249630\n",
      "Iteration 283, loss = 1.07520020\n",
      "Iteration 284, loss = 1.34960353\n",
      "Iteration 285, loss = 1.28483974\n",
      "Iteration 286, loss = 1.16878554\n",
      "Iteration 287, loss = 1.11887518\n",
      "Iteration 288, loss = 1.06859793\n",
      "Iteration 289, loss = 1.01957645\n",
      "Iteration 290, loss = 1.22089326\n",
      "Iteration 291, loss = 1.05510149\n",
      "Iteration 292, loss = 0.94863399\n",
      "Iteration 293, loss = 0.97908516\n",
      "Iteration 294, loss = 0.93262512\n",
      "Iteration 295, loss = 1.01351466\n",
      "Iteration 296, loss = 1.11052103\n",
      "Iteration 297, loss = 1.07152049\n",
      "Iteration 298, loss = 1.04173047\n",
      "Iteration 299, loss = 0.91368493\n",
      "Iteration 300, loss = 0.97569649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_p, y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20b7582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=regr.predict(X_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a166b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1426, 1948])\n",
      "torch.Size([2665, 1948])\n"
     ]
    }
   ],
   "source": [
    "print(X_c.shape)\n",
    "print(X_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3232754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 40.81013896 183.08454245   6.35860781   8.13479305 112.96478304\n",
      "   6.73762444] [ 38.69000244 182.94000244   8.98999977   6.11000013 108.02000427\n",
      "   4.97000027]\n"
     ]
    }
   ],
   "source": [
    "print(a[1],y_p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a2fca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.reshape(rows_p,cols_p,num_elements)\n",
    "for i in range(num_elements):\n",
    "    f2=plt.figure()\n",
    "    plt.imshow(b[:,:,i])\n",
    "#plt.title()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(key_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343cf1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe191b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "min_cu=int(np.min(a[:,3]))\n",
    "max_cu=int(np.max(a[:,3]))\n",
    "window_size=int(max_cu-min_cu)//100\n",
    "counts=np.zeros(100)\n",
    "for i in range(len(X_p)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c58607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea5b4247d6c7cda244ea30007fb25f3bb251da4e1f9effe2a3768bfb140f78bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
